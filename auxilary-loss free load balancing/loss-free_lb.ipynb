{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f237b6",
   "metadata": {},
   "source": [
    "## Implementation of the paper *Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a123f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb4d55",
   "metadata": {},
   "source": [
    "## Los-Free Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7f4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self,input_dim:int,hidden_dim:int):\n",
    "        \"\"\"\n",
    "        Initializes the SwiGLUFFN module.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): The dimensionality of the input features.\n",
    "            hidden_dim (int): The dimensionality of the hidden layer.\n",
    "            \n",
    "        Initializes three linear layers:\n",
    "        - `w_1`: Projects input features to the hidden dimension.\n",
    "        - `w_2`: Projects input features to the hidden dimension using a separate path.\n",
    "        - `out`: Projects the transformed hidden representation back to the input dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w_1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.w_2=nn.Linear(input_dim,hidden_dim)\n",
    "        self.out=nn.Linear(hidden_dim,input_dim)\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Computes the output of the SwiGLUFFN module.\n",
    "        \"\"\"\n",
    "        return self.out(self.w_1(x) * F.silu(self.w_2(x)))\n",
    "\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,n_experts,k):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.k=k\n",
    "        self.experts=self.experts=nn.ModuleList([SwiGLUFFN(input_dim=input_size,hidden_dim=hidden_size) for _ in range(n_experts)])\n",
    "        self.W_router=nn.Linear(in_features=input_size,out_features=n_experts)\n",
    "        self.bias=nn.Parameter(torch.zeros(n_experts), requires_grad=False)\n",
    "        self.u=0.1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        router_scores = self.W_router(x)  # (batch_size, seq_len, num_experts)\n",
    "        router_scores_biased = router_scores + self.bias.view(1, 1, -1)  \n",
    "        topk_vals, topk_indices = torch.topk(router_scores_biased, k=self.k, dim=-1)#(batch_size, seq_len, k)\n",
    "        topk_probs = F.softmax(topk_vals, dim=-1)  #(batch_size, seq_len, k)\n",
    "        all_expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=0)  #(num_experts, batch_size, seq_len, output_size)\n",
    "        indices = topk_indices.unsqueeze(-1).expand(-1, -1, -1, self.output_size)  #(batch_size, seq_len, k, output_size)\n",
    "        expert_outputs = all_expert_outputs.permute(1, 2, 0, 3)  # (batch_size, seq_len, num_experts, output_size)\n",
    "        expert_outputs = torch.gather(expert_outputs, dim=2, index=indices) \n",
    "        final_output = (expert_outputs * topk_probs.unsqueeze(-1)).sum(dim=2)  #(batch_size, seq_len, output_size)\n",
    "\n",
    "        \n",
    "        topk_mask=torch.zeros_like(router_scores)\n",
    "        topk_mask.scatter_(-1,topk_indices,1)\n",
    "        tokens_per_expert=topk_mask.sum(dim=(0,1))\n",
    "        mean_load=tokens_per_expert.mean()\n",
    "        load_violation=mean_load-tokens_per_expert# (num_experts,) \n",
    "        self.bias.data+= self.u * torch.sign(load_violation)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e892d",
   "metadata": {},
   "source": [
    "#### Test moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25757aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([4, 10, 32])\n",
      "Output shape: torch.Size([4, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_size = 32\n",
    "hidden_size = 64\n",
    "n_experts = 6\n",
    "top_k = 2\n",
    "\n",
    "moe = MOE(input_size=input_size, hidden_size=hidden_size, output_size=input_size, n_experts=n_experts, k=top_k)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "output = moe(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")     \n",
    "print(f\"Output shape: {output.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69568ced",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fa242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def clones(module,N):\n",
    "    \"\"\"\n",
    "    Create a list of N identical layers.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): A neural network module to be cloned.\n",
    "        N (int): The number of clones to create.\n",
    "\n",
    "    Returns:\n",
    "        nn.ModuleList: A list containing N deep copies of the input module.\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "  \n",
    "  \"mask: binary mask of 0 and 1s, 1-> allowed , 0->mask it\"\n",
    "  #query, key, value -> N,h,T,d_k\n",
    "  d_k=query.size(-1)\n",
    "  scores=torch.matmul(query,key.transpose(-1,-2))/math.sqrt(d_k) # N,h,T,d_k @ N,h,d_k,T = N,h,T,T\n",
    "  if mask is not None:\n",
    "    scores=scores.masked_fill(mask==0,-1e9)\n",
    "\n",
    "  p_attn=scores.softmax(dim=-1)\n",
    "  if dropout is not None:\n",
    "    p_attn=dropout(p_attn)\n",
    "\n",
    "  return torch.matmul(p_attn,value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "  def __init__(self,h,d_model,dropout=0.1):\n",
    "    \"\"\"\n",
    "    Create a MultiHeadedAttention layer.\n",
    "\n",
    "    Args:\n",
    "        h (int): The number of heads in the multi-head attention mechanism.\n",
    "        d_model (int): The number of expected features in the input.\n",
    "        dropout (float, optional): The dropout to apply to the attention weights. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    super(MultiHeadedAttention,self).__init__()\n",
    "\n",
    "    self.d_k=d_model//h\n",
    "    self.h=h\n",
    "    self.linears=clones(nn.Linear(d_model, d_model), 4)\n",
    "    self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self,query,key,value,mask=None):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the forward pass of the multi-headed attention layer.\n",
    "\n",
    "    Args:\n",
    "        query (torch.Tensor): The query tensor.\n",
    "        key (torch.Tensor): The key tensor.\n",
    "        value (torch.Tensor): The value tensor.\n",
    "        mask (torch.Tensor, optional): The mask to apply to the attention weights. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output of the forward pass.\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "      mask=mask.unsqueeze(1)\n",
    "      # print(mask.shape)\n",
    "\n",
    "    nbatches=query.shape[0]\n",
    "\n",
    "    query,key,value=[\n",
    "        lin(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2)\n",
    "        for lin,x in zip(self.linears,(query,key,value))\n",
    "    ]\n",
    "\n",
    "    x,self_attn=attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "\n",
    "    x=(x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k))\n",
    "\n",
    "    del query\n",
    "    del key\n",
    "    del value\n",
    "\n",
    "    return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c1a91",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d7a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(x, pad_token):\n",
    "    \"\"\"Create a mask to ignore padding tokens.\n",
    "    x: (N, T)\n",
    "    Returns: (N, 1, T)\n",
    "    \"\"\"\n",
    "    return (x != pad_token).unsqueeze(-2)  # N, 1, T\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"Create a causal mask to prevent attending to future tokens.\n",
    "    Returns: (1, T, T)\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return mask == 0  # 1 -> valid, 0 -> masked\n",
    "\n",
    "def combined_mask(x, pad_token):\n",
    "    \"\"\"Combine padding mask and causal mask.\n",
    "    x: (N, T)\n",
    "    Returns: (N, T, T)\n",
    "    \"\"\"\n",
    "    N, T = x.shape\n",
    "\n",
    "\n",
    "\n",
    "    # Create padding mask: (N, 1, T)\n",
    "    pad_mask_ = pad_mask(x, pad_token)\n",
    "\n",
    "    # Create causal mask: (1, T, T)\n",
    "    causal_mask_ = causal_mask(T)\n",
    "\n",
    "    # Expand causal mask to match batch size: (N, T, T)\n",
    "    causal_mask_ = causal_mask_.expand(N, -1, -1)\n",
    "\n",
    "    # Apply padding mask to the causal mask\n",
    "    # For each query position (row), if it's a padding token, the entire row should be False\n",
    "    combined_mask_ = pad_mask_.transpose(-1, -2) & causal_mask_\n",
    "\n",
    "    return combined_mask_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce50d46",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea77b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayerConnection(nn.Module):\n",
    "  def __init__(self,d_model,dropout):\n",
    "    \"\"\"\n",
    "  Initialize a sublayer connection layer.\n",
    "  \n",
    "  Parameters:\n",
    "  d_model (int): The number of expected features in the input.\n",
    "  dropout (float): The amount of dropout to apply.\n",
    "    \"\"\"\n",
    "    super(SubLayerConnection,self).__init__()\n",
    "    self.norm=nn.LayerNorm(d_model)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,x,sublayer):\n",
    "\n",
    "    return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "\n",
    "  def __init__(self,self_attn,dropout,n_experts=8,k=2,hidden=768*4,h=12,d_model=768):\n",
    "\n",
    "    \"\"\"\n",
    "  Initialize a GPT layer.\n",
    "  \n",
    "  Parameters:\n",
    "  self_attn (nn.Module): A multi-headed self-attention layer.\n",
    "  feed_forward (nn.Module): A MOE feed-forward layer.\n",
    "  dropout (float): The amount of dropout to apply.\n",
    "  hidden (int): The number of neurons in the hidden layer of the feed-forward layer. Defaults to 768*4.\n",
    "  h (int): The number of attention heads. Defaults to 12.\n",
    "  d_model (int): The number of expected features in the input. Defaults to 768.\n",
    "  \"\"\"\n",
    "    super().__init__()\n",
    "    self.self_attn=self_attn\n",
    "    self.h=h\n",
    "    self.feed_forward=MOE(input_size=d_model,hidden_size=hidden,output_size=d_model,n_experts=n_experts,k=k)\n",
    "    self.sublayer=clones(SubLayerConnection(d_model,dropout),2)\n",
    "    self.d_model=d_model\n",
    "\n",
    "\n",
    "  def forward(self,x,mask=None):\n",
    "    if mask is not None:\n",
    "      x=self.sublayer[0](x, lambda x:self.self_attn(x,x,x,mask))\n",
    "    else:\n",
    "      x=self.sublayer[0](x, lambda x:self.self_attn(x,x,x,mask=None))\n",
    "    return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b244b9",
   "metadata": {},
   "source": [
    "##### test cases for layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c30af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size, seq_len, d_model = 2, 3, 4\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mask = None\n",
    "\n",
    "self_attn = MultiHeadedAttention(h=4,d_model=4)\n",
    "layer_test = Layer(d_model=d_model, self_attn=self_attn, dropout=0.1)\n",
    "\n",
    "output = layer_test(x, mask)\n",
    "print(output.shape)\n",
    "assert output.shape == x.shape, \"❌ Layer output shape mismatch\"\n",
    "\n",
    "\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "pad_token = 0\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "token_ids = torch.randint(1, 10, (batch_size, seq_len))  # Simulated token indices\n",
    "mask = combined_mask(token_ids, pad_token)\n",
    "\n",
    "self_attn = MultiHeadedAttention(h=4,d_model=d_model)\n",
    "layer = Layer(d_model=d_model, self_attn=self_attn, dropout=0.1)\n",
    "\n",
    "output = layer(x, mask)\n",
    "\n",
    "assert output.shape == x.shape, \"❌ Layer output shape mismatch\"\n",
    "assert not torch.isnan(output).any(), \"❌ Output contains NaNs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af09c9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layer, vocab=50257, max_seq_len=1024, N=12, d_model=768, h=12, k=2, n_experts=8, hidden=768*4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            layer (nn.Module): The type of layer to use in the model.\n",
    "            vocab (int): The size of the input vocabulary.\n",
    "            max_seq_len (int): The maximum sequence length supported by the model. Defaults to 1024.\n",
    "            N (int): The number of layers to use. Defaults to 12.\n",
    "            d_model (int): The number of features in the input and output. Defaults to 768.\n",
    "            h (int): The number of attention heads to use. Defaults to 12.\n",
    "            hidden (int): The number of neurons in the hidden layer of the feed-forward layer. Defaults to 768 * 4.\n",
    "            dropout (float): The dropout probability to use. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)  # clones\n",
    "        self.norm = nn.LayerNorm(d_model)  # Fix: Use d_model, not layer.size\n",
    "        self.embedding = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.pos = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lm_head=nn.Linear(d_model,vocab)\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for the model.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=0.02)  # Normal init with std=0.02\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()  # Bias = 0\n",
    "\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=0.02)  # Normal init with std=0.02\n",
    "\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.weight.data.fill_(1.0)  # LayerNorm weight = 1\n",
    "                module.bias.data.zero_()  # LayerNorm bias = 0\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N, T = x.shape\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0)  # (1, T)\n",
    "        token_embeddings = self.embedding(x)  # (N, T, d_model)\n",
    "        position_embeddings = self.pos(positions)  # (1, T, d_model)\n",
    "        x = token_embeddings + position_embeddings  # (N, T, d_model)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) if mask is not None else layer(x)\n",
    "        \n",
    "        logits=self.lm_head(self.norm(x)) \n",
    "        print('logit_shape',logits.shape) # Apply final LayerNorm\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c259b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_shape torch.Size([2, 4, 1000])\n",
      "torch.Size([2, 4, 1000])\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size, seq_len, d_model = 2, 4, 768  # Matching d_model init\n",
    "pad_token = 0\n",
    "vocab=1000\n",
    "x = torch.randint(0, 1024, (batch_size, seq_len))\n",
    "token_ids = torch.randint(1, 10, (batch_size, seq_len))  # No padding\n",
    "mask = combined_mask(token_ids, pad_token)\n",
    "self_attn = MultiHeadedAttention(h=4, d_model=d_model)\n",
    "layer = Layer(d_model=d_model, self_attn=self_attn, dropout=0.1,n_experts=8,k=2,hidden=768*4)\n",
    "\n",
    "model = Model(layer=layer, N=8,vocab=vocab, d_model=d_model)\n",
    "output = model(x, mask=mask)\n",
    "print(output.shape)\n",
    "assert output.shape == (batch_size, seq_len, vocab), \"❌ Output shape mismatch\"\n",
    "assert not torch.isnan(output).any(), \"❌ Output contains NaNs\"\n",
    "\n",
    "print(\"Passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
