{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4daedaa",
   "metadata": {},
   "source": [
    "## Converting Images to Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edfe04",
   "metadata": {},
   "source": [
    "### Singular image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "819c7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f42d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 16\n",
      "torch.Size([16, 8, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "img_1=torch.randn((32,32,3)) # H,W,C\n",
    "H,W,C=img_1.shape\n",
    "patch_size=8\n",
    "N=int((H*W)/(patch_size**2))\n",
    "print(\"N:\",N)\n",
    "# H,W,C -> N,P^2,C\n",
    "# 32,32,3 -> 16,8^2,3 -> 16 patches, each of size 8 x 8 x 3\n",
    "img1_patch=img_1.view(N,patch_size,patch_size,C)\n",
    "print(img1_patch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54e4a0",
   "metadata": {},
   "source": [
    "#### Why using \"view\" is incorrect here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ccb7d",
   "metadata": {},
   "source": [
    "**What you want to do**\n",
    "- You have an image of 32,32,3 (HWC) and wish to split it into **non-overlapping square patches** of size 8x8 where each patch *preserves a spatial neighborhood*\n",
    "- So, after patching you want 16 patches of shape 8x8x3  \n",
    "  \n",
    "**What’s wrong with view(N, patch_size, patch_size, C):**\n",
    "- `img1_patch = img_1.view(N, patch_size, patch_size, C)`  \n",
    "- This **does not extract actual patches from the image**\n",
    "- Instead, view() just reshapes the tensor in memory, without **caring about spatial structure**\n",
    "- This means,\n",
    "    - It takes a whole image as a long 1D stream of numbers, and just chunks it *blindly* into blocks of shape 8,8,3\n",
    "    - This ignores where each pixel is actually located in the image\n",
    "    - So your \"patches\"  are made of **random pixel groupings** that are **not spatially connected** like the top left 8x8 region, top-right etc\n",
    "- It's like cutting a photo into rectangles **without caring where the cuts go**\n",
    "  \n",
    "`view` = **reshape blindly\n",
    "- “Hey, take this long row of numbers in memory and chunk it up into a new shape, as long as the total number of elements stays the same.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3ca42",
   "metadata": {},
   "source": [
    "#### use `torch.nn.Unfold`\n",
    "- Sliding window extractor\n",
    "- Slices tensor into **overlapping or non-overlapping** chunks based on stride and stacks them into a new dim\n",
    "- It keeps **spatial groups** intact. \n",
    "- Unlike .view() or .reshape(), which blindly reshuffle numbers, unfold preserves the structure of each patch, perfect for feeding into ViT’s linear projection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c97b181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1=torch.randn((3,32,32)) # H,W,C\n",
    "C,H,W=img_1.shape\n",
    "patch_size=8\n",
    "patch_1=img_1.unfold(1,patch_size,patch_size).unfold(2,patch_size,patch_size)\n",
    "patch_1.shape # (C, num_patches_H, num_patches_W, patch_size, patch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cab156",
   "metadata": {},
   "source": [
    "`img_1.unfold(1,patch_size,patch_size).unfold(2,patch_size,patch_size)`\n",
    "- Imagine you're cutting brownies in a tray\n",
    "- First, you make a **horizontal cut** (height)\n",
    "- Then you make a **vertical cut** (width)\n",
    "- Now you've got perfect little squares (patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4966ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next\n",
    "# (C, num_patches_H, num_patches_W, patch_size, patch_size) -> (num_patches, C, patch_size,patch_size)\n",
    "# Can i use view(-1,192)?\n",
    "patch_1=patch_1.permute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25897d32",
   "metadata": {},
   "source": [
    "**permute vs view**  \n",
    "\n",
    "Let's say we have a book that has:\n",
    "- 2 chapters\n",
    "- Each chapter has 3 pages\n",
    "- Each chapter has 4 lines   \n",
    "`x = torch.randn(2, 3, 4)  # (chapters, pages, lines)`  \n",
    "`permute`\n",
    "- rotate the dimensions around\n",
    "- No reshaping of data - just reorder how you see the tensor in terms of axis\n",
    "- permute(1,0,2)\n",
    "    - *I wanna reorder this book so i see all the pages first, grouped by chapter*\n",
    "    - havent changed the number of pages or files\n",
    "    - Just **how you organize** your view of the book\n",
    "  \n",
    "`view`\n",
    "- Changes the **shape** without changing the order\n",
    "- \"hey pytorch, treat the same memory as a new shape - dont shuffle anything\"\n",
    "- view(6,4)\n",
    "    - *Forget chapters and pages, just give me a list of 6 flat sections, each with 4 lines.*\n",
    "    - **You’re flattening or reshaping, but not caring about what those lines originally belonged to.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fa9a5",
   "metadata": {},
   "source": [
    "> In the context of ViT, we **deeply care about the structure and order** of how patches are extracted and processed\n",
    "- view would just smash the memory together without respecting spatial grouping of each patch\n",
    "-permute ensures each row ends up representing one patch, fully and correctly ordered.\n",
    "-  Use permute because\n",
    "    - \tIt reorganizes the tensor to make each patch’s data contiguous and aligned\n",
    "    - So that each patch stays intact when you finally flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e727477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 192])\n"
     ]
    }
   ],
   "source": [
    "img_1=torch.randn((3,32,32)) # H,W,C\n",
    "C,H,W=img_1.shape\n",
    "patch_size=8\n",
    "patch_1=img_1.unfold(1,patch_size,patch_size).unfold(2,patch_size,patch_size)\n",
    "patch_1.shape # (C, num_patches_H, num_patches_W, patch_size, patch_size)\n",
    "\n",
    "patch_1=patch_1.permute(1,2,0,3,4).reshape(-1,patch_size*patch_size*C)\n",
    "print(patch_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b3bde",
   "metadata": {},
   "source": [
    "### Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4259720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 4, 4, 8, 8])\n",
      "torch.Size([10, 16, 192])\n"
     ]
    }
   ],
   "source": [
    "imgs=torch.randn((10,3,32,32)) # (batch_size,C,H,W)\n",
    "B,C,H,W=imgs.shape\n",
    "patch_size=8\n",
    "patches=imgs.unfold(2,patch_size,patch_size).unfold(3,patch_size,patch_size) \n",
    "print(patches.shape) #(batch_size,C,num_patches_H,num_patches_W,patch_size,patch_size)\n",
    "patches=patches.permute(0,2,3,1,4,5).reshape(B,-1,patch_size*patch_size*C)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a516d",
   "metadata": {},
   "source": [
    "## CLS token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51728ad",
   "metadata": {},
   "source": [
    "### Singular image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3570ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1=torch.randn((16,192)) \n",
    "D=int(192*4)\n",
    "linear=nn.Linear(in_features=img_1.shape[-1],out_features=D)\n",
    "img_1=linear(img_1)\n",
    "img_1.shape #16,768\n",
    "cls=nn.Parameter(torch.randn((1,D))) #1,768\n",
    "print(cls.shape)\n",
    "input_1=torch.cat((cls,img_1),dim=0)\n",
    "input_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2ea92",
   "metadata": {},
   "source": [
    "### Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75f4e3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs=torch.randn((10,16,192))\n",
    "D=int(192*4)\n",
    "\n",
    "linear=nn.Linear(in_features=imgs.shape[-1],out_features=D)\n",
    "imgs=linear(imgs)\n",
    "imgs.shape #10,16,768\n",
    "cls=nn.Parameter(torch.randn((1,D))) #1,768\n",
    "#one cls token per image\n",
    "cls=cls.expand(imgs.shape[0],1,D)\n",
    "inputs=torch.cat((cls,imgs),dim=1)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9a62d",
   "metadata": {},
   "source": [
    "## Extracting the CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0469e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
