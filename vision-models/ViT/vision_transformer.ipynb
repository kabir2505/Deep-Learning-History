{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099f6034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9540f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3f93c",
   "metadata": {},
   "source": [
    "## Patching + linear_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a1f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Images (B,C,H,W)\n",
    "    Output: (B,N+1,d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self,C,patch_size=16,d_model=786):\n",
    "        super().__init__()\n",
    "        self.patch_size=patch_size\n",
    "        self.C=C\n",
    "        self.in_=int((patch_size**2)*self.C)\n",
    "        self.linear=nn.Linear(in_features=self.in_,out_features=d_model)\n",
    "    \n",
    "    def forward(self,img):\n",
    "        B,C,H,W=img.shape\n",
    "        patches=img.unfold(2,self.patch_size,self.patch_size).unfold(3,self.patch_size,self.patch_size) # (B,C, num_patches_H, num_patches_W, patch_size, patch_size)\n",
    "        patches=patches.permute(0,2,3,1,4,5).reshape(B,-1,self.in_)\n",
    "        #N,P**2C\n",
    "        \n",
    "        return self.linear(patches) # B,N,d_model\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3177f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Patch_test=Patching(C=3,patch_size=16,d_model=768)\n",
    "img_test=torch.randn((10,3,224,224))\n",
    "Patch_test(img_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44fcb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_test(nn.Module):\n",
    "    def __init__(self,H,W,C,patch_size=16,d_model=786):\n",
    "        super().__init__()\n",
    "        self.C=C\n",
    "        self.patch_size=patch_size\n",
    "        self.num_patches=int((H*W)/(self.patch_size**2))\n",
    "        self.d_model=d_model\n",
    "        self.pos_embed=nn.Parameter(torch.randn(1,self.num_patches+1,d_model))\n",
    "        self.cls_token=nn.Parameter(torch.zeros(1,d_model)) #1,d_model\n",
    "        self.patching=Patching(C=self.C,patch_size=self.patch_size,d_model=self.d_model)\n",
    "    def forward(self,imgs):\n",
    "        imgs=self.patching(imgs) #B,N,d_model\n",
    "        cls_token=self.cls_token.expand(imgs.shape[0],-1,-1)\n",
    "        imgs=torch.cat((cls_token,imgs),dim=1) # B,N+1,d_model\n",
    "        imgs=imgs + self.pos_embed\n",
    "        return imgs\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbd9962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 197, 786])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_test=ViT_test(H=224,W=224,C=3)\n",
    "vit_test(img_test).shape # B,num_patches+1,d_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da96059",
   "metadata": {},
   "source": [
    "## Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2cbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def clones(module,N):\n",
    "    \"\"\"\n",
    "    Create a list of N identical layers.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): A neural network module to be cloned.\n",
    "        N (int): The number of clones to create.\n",
    "\n",
    "    Returns:\n",
    "        nn.ModuleList: A list containing N deep copies of the input module.\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def attention(query,key,value,dropout=0):\n",
    "  \n",
    " \n",
    "  #query, key, value -> N,h,T,d_k\n",
    "  d_k=query.size(-1)\n",
    "  scores=torch.matmul(query,key.transpose(-1,-2))/math.sqrt(d_k) # N,h,T,d_k @ N,h,d_k,T = N,h,T,T\n",
    "  \n",
    "  p_attn=scores.softmax(dim=-1)\n",
    "  if dropout is not None:\n",
    "    p_attn=dropout(p_attn)\n",
    "\n",
    "  return torch.matmul(p_attn,value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "  def __init__(self,h,d_model,dropout=0):\n",
    "    \"\"\"\n",
    "    Create a MultiHeadedAttention layer.\n",
    "\n",
    "    Args:\n",
    "        h (int): The number of heads in the multi-head attention mechanism.\n",
    "        d_model (int): The number of expected features in the input.\n",
    "        dropout (float, optional): The dropout to apply to the attention weights. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    super(MultiHeadedAttention,self).__init__()\n",
    "\n",
    "    self.d_k=d_model//h\n",
    "    self.h=h\n",
    "    self.linears=clones(nn.Linear(d_model, d_model), 4)\n",
    "    self.dropout=nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self,query,key,value):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the forward pass of the multi-headed attention layer.\n",
    "\n",
    "    Args:\n",
    "        query (torch.Tensor): The query tensor.\n",
    "        key (torch.Tensor): The key tensor.\n",
    "        value (torch.Tensor): The value tensor.\n",
    "       \n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output of the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    nbatches=query.shape[0]\n",
    "\n",
    "    query,key,value=[\n",
    "        lin(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2)\n",
    "        for lin,x in zip(self.linears,(query,key,value))\n",
    "    ]\n",
    "\n",
    "    x,self_attn=attention(query,key,value,dropout=self.dropout)\n",
    "\n",
    "    x=(x.transpose(1,2).contiguous().view(nbatches,-1,self.h*self.d_k))\n",
    "\n",
    "    del query\n",
    "    del key\n",
    "    del value\n",
    "\n",
    "    return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5607d3",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57c7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,d_model,dropout=0):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(p=dropout)  \n",
    "        self.linear1=nn.Linear(in_features=d_model,out_features=4*d_model)\n",
    "        self.linear2=nn.Linear(in_features=4*d_model,out_features=d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.dropout(F.gelu(x))\n",
    "        x=self.linear2(x)\n",
    "        x=self.dropout(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5203f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "mlp_test = MLP(d_model=768, dropout=0.1)\n",
    "\n",
    "x_test = torch.randn(10, 197, 768)  # B=10, N=197, D=768\n",
    "\n",
    "out = mlp_test(x_test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a261b2",
   "metadata": {},
   "source": [
    "### residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a95258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayerConnection(nn.Module):\n",
    "  def __init__(self,d_model,dropout):\n",
    "    \"\"\"\n",
    "  Initialize a sublayer connection layer.\n",
    "  \n",
    "  Parameters:\n",
    "  d_model (int): The number of expected features in the input.\n",
    "  dropout (float): The amount of dropout to apply.\n",
    "    \"\"\"\n",
    "    super(SubLayerConnection,self).__init__()\n",
    "    self.norm=nn.LayerNorm(d_model)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,x,sublayer):\n",
    "\n",
    "    return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "227c0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_layer(nn.Module):\n",
    "\n",
    "  def __init__(self,self_attn,mlp,dropout,hidden=768*4,h=12,d_model=768):\n",
    "\n",
    "    \"\"\"\n",
    "  Initialize a ViT_layer\n",
    "  \n",
    "  Parameters:\n",
    "  self_attn (nn.Module): A multi-headed self-attention layer.\n",
    "  feed_forward (nn.Module): A feed-forward layer.\n",
    "  dropout (float): The amount of dropout to apply.\n",
    "  hidden (int): The number of neurons in the hidden layer of the feed-forward layer. Defaults to 768*4.\n",
    "  h (int): The number of attention heads. Defaults to 12.\n",
    "  d_model (int): The number of expected features in the input. Defaults to 768.\n",
    "  \"\"\"\n",
    "    super().__init__()\n",
    "    self.self_attn=self_attn\n",
    "    self.h=h\n",
    "    self.mlp=mlp\n",
    "    self.sublayer=clones(SubLayerConnection(d_model,dropout),2)\n",
    "    self.d_model=d_model\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "  \n",
    "    x=self.sublayer[0](x, lambda x:self.self_attn(x,x,x))\n",
    "    return self.sublayer[1](x, self.mlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd72ac74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 197, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_test=ViT_layer(self_attn=MultiHeadedAttention(h=12,d_model=768),mlp=MLP(d_model=768),dropout=0.1)\n",
    "layer_test(x_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4dcfa",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e508106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,n_classes=1000,H=224,W=224,C=3,patch_size=16,d_model=768):\n",
    "        super().__init__()\n",
    "        self.H=H\n",
    "        self.W=W\n",
    "        self.C=C\n",
    "        self.patch_size=patch_size\n",
    "        self.d_model=d_model\n",
    "        self.num_patches=int((H*W)/(self.patch_size**2))\n",
    "        self.pos_embed=nn.Parameter(torch.randn(1,self.num_patches+1,d_model))\n",
    "        self.cls_token=nn.Parameter(torch.zeros(1,d_model))\n",
    "        self.patching=Patching(C=self.C,patch_size=self.patch_size,d_model=self.d_model)\n",
    "        self.layers=clones(ViT_layer(self_attn=MultiHeadedAttention(h=12,d_model=self.d_model),mlp=MLP(d_model=self.d_model),dropout=0.1),12)\n",
    "        self.norm=nn.LayerNorm(self.d_model)\n",
    "        self.mlp_head=nn.Linear(in_features=self.d_model,out_features=n_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.patching(x)\n",
    "        x=torch.cat((self.cls_token.expand(x.shape[0],-1,-1),x),dim=1)\n",
    "        x=x+self.pos_embed\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        x=self.norm(x)\n",
    "        cls_token_final=x[:,0] #batch_size,d_model\n",
    "        x=self.mlp_head(cls_token_final) #batch_size,n_classes\n",
    "        return x,cls_token_final #batch_size,n_classes batch_size,d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c748e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 1000])\n",
      "CLS Token Output shape: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "model_test = ViT(\n",
    "    n_classes=1000,\n",
    "    H=224,\n",
    "    W=224,\n",
    "    C=3,\n",
    "    patch_size=16,\n",
    "    d_model=768\n",
    ")\n",
    "\n",
    "# Random test input: batch of 8 RGB images of size 224x224\n",
    "x_test = torch.randn(8, 3, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "logits, cls_token_output = model_test(x_test)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Logits shape:\", logits.shape)               # Expected: [8, 1000]\n",
    "print(\"CLS Token Output shape:\", cls_token_output.shape)  # Expected: [8, 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d65760",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c5bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([4, 1000])\n",
      "CLS Token Output shape: torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "x_test=torch.randn(4,3,224,224)\n",
    "logits, cls_token_output=model_test(x_test)\n",
    "print(\"Logits shape:\", logits.shape)              \n",
    "print(\"CLS Token Output shape:\", cls_token_output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ead9f9",
   "metadata": {},
   "source": [
    "## Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "187d8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5a3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagenet statistics \n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def53b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms.v2._geometry.RandomResizedCrop"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.RandomResizedCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04793a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformatons(image_size=(224,224),image_mean=IMAGENET_MEAN,image_std=IMAGENET_STD\n",
    "                         , hflip_probab=0.5, interpolation=InterpolationMode.BILINEAR, random_aug_magnitude=9\n",
    "                         ):\n",
    "    #interpolation: How to resize image when cropping/resizing. BILINEAR is smooth and preferred for ViTs.\n",
    "    transformation_chain=[]\n",
    "    \n",
    "    transformation_chain.append(\n",
    "        v2.RandomResizedCrop(image_size,interpolation=interpolation,antialias=True)\n",
    "        )\n",
    "    \n",
    "    if hflip_probab > 0:\n",
    "        transformation_chain.append(v2.RandomHorizontalFlip(p=hflip_probab)) #flips image horizontaly with 50% probab\n",
    "    \n",
    "    if random_aug_magnitude > 0:\n",
    "        print(\"Enabling Random Augmentation!\")\n",
    "        transformation_chain.append(v2.RandAugment(magnitude=rand_aug_magnitude, interpolation=interpolation))\n",
    "        #Applies random transformations (e.g., brightness, contrast, shear, rotation).\n",
    "        #\tMagnitude controls how strong the augmentations are.\n",
    "        #Makes your dataset more diverse → better generalization.\n",
    "        \n",
    "    \n",
    "    transformation_chain.append(v2.PILToTensor()) #Converts from PIL Image to PyTorch Tensor.\n",
    "    transformation_chain.append(v2.ToDtype(torch.float32,scale=True)) #scales pixels values from [0,255] to [0.0,1.0]\n",
    "    transformation_chain.append(v2.Normalize(mean=image_mean,std=img_std)) # pixel-mean/std\n",
    "    #brings each channel to zero-mean, unit variance. Essential for fast training and stability\n",
    "    \n",
    "    return transforms.Compose(trasformation_chain)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "569ffc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_transformations(image_size=(224,224), resize_image=(256,256), image_mean=IMAGENET_MEAN, image_std=IMAGENET_STD,\n",
    "                         interpolation=InterpolationMode.BILINEAR\n",
    "                         ):\n",
    "    \n",
    "    #meant to be used during evaluation or inference, not training\n",
    "    transformations=transforms.Compose([\n",
    "        v2.Resize(resize_image,interpolation=interpolation, antialias=True),\n",
    "        v2.CenterCrop(image_size),\n",
    "        v2.PILToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=image_mean, std=image_std)\n",
    "    ])\n",
    "    \n",
    "    return transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a454fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "# customizing the collat_fn of the dataloader\n",
    "#blend images after theyve been batched together\n",
    "#but before sending them to the model\n",
    "def mixup_cutmix_collate_fn(mixup_alpha=0.2, cutmix_alpha=1.0, num_classes=1000):\n",
    "    #returns a custom collate fn for the dataloader\n",
    "    #mixup_alpha: the probability of applying mixup\n",
    "    #cutmix_alpha: the probability of applying cutmix\n",
    "    #num_classes: the number of classes in the dataset\n",
    "    \n",
    "    mix_cut_transforms=None\n",
    "    mixup_cutmix=[]\n",
    "    \n",
    "    if mixup_alpha > 0:\n",
    "        print(\"Enabling Mixup!\")\n",
    "        mixup_cutmix.append(v2.MixUp(alpha=mixup_alpha,num_classes=num_classes))\n",
    "    \n",
    "    if cutmix_alpha > 0:\n",
    "        print(\"Enabling Cutmix!\")\n",
    "        mixup_cutmix.append(v2.CutMix(alpha=cutmix_alpha,num_classes=num_classes))\n",
    "    \n",
    "    if len(mixup_cutmix) > 0:\n",
    "        mix_cut_transforms=v2.RandomChoice(mixup_cutmix)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        collated=default_collate(batch)\n",
    "        \n",
    "        if mix_cut_transforms is not None:\n",
    "            collated=mix_cut_transforms(collated)\n",
    "        \n",
    "        return collated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54e6a619",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'zh-plus/tiny-imagenet' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzh-plus/tiny-imagenet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ds\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/datasets/load.py:1853\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1852\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1853\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/datasets/load.py:1717\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/datasets/load.py:1643\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1645\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m   1646\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1647\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m   1651\u001b[0m     )\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'zh-plus/tiny-imagenet' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"zh-plus/tiny-imagenet\",split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5af9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
