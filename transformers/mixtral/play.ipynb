{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b3c7da",
   "metadata": {},
   "source": [
    "## Sliding window attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4969ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d38323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4, 32, 5])\n",
      "torch.Size([12, 4, 32, 5])\n",
      "torch.Size([12, 4, 32, 14])\n"
     ]
    }
   ],
   "source": [
    "batch_size=12\n",
    "d_model=56\n",
    "seq_len=32\n",
    "n_heads=4\n",
    "d_head=14\n",
    "\n",
    "window_size=5\n",
    "\n",
    "x=torch.rand(batch_size,seq_len,d_model)\n",
    "W_q=nn.Linear(in_features=d_model,out_features=d_model)\n",
    "W_k=nn.Linear(in_features=d_model,out_features=d_model)\n",
    "W_v=nn.Linear(in_features=d_model,out_features=d_model)\n",
    "\n",
    "q=W_q(x).view(batch_size,seq_len,n_heads,d_head).transpose(1,2)\n",
    "k=W_k(x).view(batch_size,seq_len,n_heads,d_head).transpose(1,2)\n",
    "v=W_v(x).view(batch_size,seq_len,n_heads,d_head).transpose(1,2)\n",
    "assert q.shape==k.shape\n",
    "#print(q.shape) # batch_size, n_heads, seq_len, d_head\n",
    "half_window=window_size//2\n",
    "pad_k=F.pad(k,(0,0,half_window,half_window))\n",
    "pad_v=F.pad(v,(0,0,half_window,half_window))\n",
    "#print(pad_k.shape) # batch_size, n_heads, seq_len+2*half_window, d_head\n",
    "\n",
    "k_unf=pad_k.unfold(dimension=2,size=window_size,step=1).transpose(3,4) #(batch_size,n_heads,seq_len,window_size,d_head)\n",
    "v_unf=pad_v.unfold(dimension=2,size=window_size,step=1).transpose(3,4)#(batch_size,n_heads,seq_len,window_size,d_head)\n",
    "#print('k_unf',k_unf.shape)\n",
    "#print('q',q.shape)\n",
    "q=q.unsqueeze(-2)\n",
    "#print('q',q.shape)\n",
    "\n",
    "attn_scores=einops.einsum(q,k_unf,'b h s w d, b h s w d -> b h s w ')\n",
    "#print(attn_scores.shape) # batch_size, n_heads, seq_len, window_size\n",
    "mask=torch.tril(torch.ones(window_size,window_size))\n",
    "mask=mask[-1]\n",
    "mask=mask.view(1,1,1,window_size).expand(batch_size,n_heads,seq_len,window_size)\n",
    "print(mask.shape)\n",
    "print(attn_scores.shape)\n",
    "attn_scores=attn_scores.masked_fill(mask==0,-float('inf'))\n",
    "attn_scores=attn_scores/(d_head**0.5)\n",
    "\n",
    "attn_weights=F.softmax(attn_scores,dim=-1)\n",
    "\n",
    "attn_output=einops.einsum(attn_weights,v_unf,'b h s w, b h s w d ->b h s d')\n",
    "print(attn_output.shape)\n",
    "attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, n_heads * d_head)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658610e",
   "metadata": {},
   "source": [
    "## Grouped Query Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7c02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(12,16,112)\n",
    "n_heads=8\n",
    "n_kv_heads=2\n",
    "d_head=14\n",
    "d_model=112\n",
    "batch_size=12\n",
    "seq_len=16\n",
    "#each 8//2=4 query group shares 1 kv head\n",
    "\n",
    "W_q=nn.Linear(d_model,n_heads*d_head)\n",
    "W_k=nn.Linear(d_model,n_kv_heads*d_head)\n",
    "W_v=nn.Linear(d_model,n_kv_heads*d_head)\n",
    "q=W_q(x).view(batch_size,seq_len,n_heads,d_head)\n",
    "k=W_k(x).view(batch_size,seq_len,n_kv_heads, d_head)\n",
    "v=W_v(x).view(batch_size,seq_len,n_kv_heads,d_head)\n",
    "\n",
    "\n",
    "n=n_heads//n_kv_heads\n",
    "k=torch.repeat_interleave(k,dim=2,repeats=n) #  batch,seq_len,n_heads,d_head\n",
    "v=torch.repeat_interleave(v,dim=2,repeats=n)#  batch,seq_len,n_heads,d_head\n",
    "\n",
    "#transpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52606cb8",
   "metadata": {},
   "source": [
    "## KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b1bdbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mcache_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_q(x)\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.cache_k=None\n",
    "self.cache_v=None\n",
    "\n",
    "q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "q=q.transpose(1,2)\n",
    "k=k.transpose(1,2)\n",
    "v=v.transpose(1,2)\n",
    "\n",
    "if is_training:\n",
    "    attn_scores=torch.matmul(q,k.transpose(-1,-2)) / (self.head_dim**0.5)\n",
    "    attn_probs=nn.Softmax(dim=-1)(attn_scores)\n",
    "    output = torch.matmul(attn_probs, v)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "else:\n",
    "    if self.cache_k is not None and self.cache_v is not None:\n",
    "        k=torch.cat((self.cache_k,k),dim=2)\n",
    "        v=torch.cat((self.cache_v,v),dim=2)\n",
    "    \n",
    "    attn_scores=torch.matmul(q,k.transpose(-1,-2)) / (self.head_dim**0.5)\n",
    "    attn_probs=nn.Softmax(dim=-1)(attn_scores)\n",
    "    output = torch.matmul(attn_probs, v)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "    self.cache_k=k\n",
    "    self.cache_v=v\n",
    "def reset_cache(self):\n",
    "    \"\"\"Clear cached keys and values after each sequence generation (for inference).\"\"\"\n",
    "    self.cache_k = None\n",
    "    self.cache_v = None\n",
    "\n",
    "#model.reset_cache() at the beginning of inference   and under range(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithKVCache(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Projection layers\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "\n",
    "        # Initialize KV cache\n",
    "        self.register_buffer('cache_k', torch.zeros(\n",
    "            (1, max_seq_len, num_heads, self.head_dim)  # batch=1 for simplicity\n",
    "        ))\n",
    "        self.register_buffer('cache_v', torch.zeros(\n",
    "            (1, max_seq_len, num_heads, self.head_dim)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0, is_training: bool = False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project queries, keys, values\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if is_training:\n",
    "            # Training mode - full attention\n",
    "            attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, v)\n",
    "        else:\n",
    "            # Inference mode - use KV cache\n",
    "            # Update cache\n",
    "            self.cache_k[:batch_size, start_pos:start_pos+seq_len] = k\n",
    "            self.cache_v[:batch_size, start_pos:start_pos+seq_len] = v\n",
    "            \n",
    "            # Get keys/values up to current position\n",
    "            keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "            \n",
    "            # Compute attention\n",
    "            attn_scores = torch.matmul(q, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # Apply causal mask for new tokens\n",
    "            if seq_len > 1:  # Only needed when processing multiple new tokens\n",
    "                mask = torch.ones((seq_len, start_pos+seq_len), dtype=torch.bool, device=x.device)\n",
    "                mask = torch.tril(mask, diagonal=start_pos)\n",
    "                attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, values)\n",
    "\n",
    "        # Output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "\n",
    "    def reset_cache(self):\n",
    "        \"\"\"Reset the KV cache between sequences\"\"\"\n",
    "        self.cache_k.zero_()\n",
    "        self.cache_v.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93897314",
   "metadata": {},
   "source": [
    "## Sliding Window, GQA and  KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dace75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AttentionWithKVCache(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int = 2048, num_kv_heads:int=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_kv_heads=num_kv_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.repeats=num_heads//num_kv_heads\n",
    "        \n",
    "        \n",
    "        # Projection layers\n",
    "        self.W_q = nn.Linear(dim,self.num_heads*self.head_dim)\n",
    "        self.W_k = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_v = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "\n",
    "        # Initialize KV cache\n",
    "        # self.register_buffer('cache_k', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)  # batch=1 for simplicity\n",
    "        # ))\n",
    "        # self.register_buffer('cache_v', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)\n",
    "        # ))\n",
    "        \n",
    "        self.cache_k=None\n",
    "        self.cache_v=None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project queries, keys, values\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            # Training mode - full attention\n",
    "            q=q.transpose(1,2)\n",
    "            k=k.transpose(1,2)\n",
    "            v=v.transpose(1,2)\n",
    "            k=torch.repeat_interleave(k,dim=1,repeats=self.repeats) #batch_size,num_heads,seq_len,head_dim\n",
    "            v=torch.repeat_interleave(v,dim=1,repeats=self.repeats)#batch_size,num_heads,seq_len,head_dim\n",
    "           \n",
    "            attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim) #batch_size,num_heads,seq_len,seq_len\n",
    "            \n",
    "            mask = torch.tril(torch.ones((seq_len, seq_len), device=x.device)).unsqueeze(0).unsqueeze(0)  # (1,1,seq_len,seq_len)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, v)\n",
    "        else:\n",
    "            # Inference mode - use KV cache\n",
    "            # Update cache\n",
    "            if self.cache_k is None or self.cache_v is None:\n",
    "                self.cache_k=torch.zeros((batch_size,self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "                self.cache_v=torch.zeros((batch_size,self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "            print(k.shape)\n",
    "            print(self.cache_k.shape)\n",
    "            self.cache_k[:batch_size, start_pos:start_pos+seq_len] = k\n",
    "            self.cache_v[:batch_size, start_pos:start_pos+seq_len] = v\n",
    "            \n",
    "            # Get keys/values up to current position\n",
    "\n",
    "            keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "            \n",
    "            q=q.transpose(1,2)\n",
    "            keys=keys.transpose(1,2)\n",
    "            values=values.transpose(1,2)            # Compute attention\n",
    "            keys=keys.repeat_interleave(self.repeats,dim=1)\n",
    "            values=values.repeat_interleave(self.repeats,dim=1)\n",
    "            print(keys.shape)\n",
    "            print(q.shape)\n",
    "            attn_scores = torch.matmul(q, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # Apply causal mask for new tokens\n",
    "            if seq_len > 1:  # Only needed when processing multiple new tokens\n",
    "                mask = torch.ones((seq_len, start_pos+seq_len), dtype=torch.bool, device=x.device)\n",
    "                mask = torch.tril(mask, diagonal=start_pos)\n",
    "                attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, values)\n",
    "\n",
    "        # Output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "\n",
    "    def reset_cache(self):\n",
    "        \"\"\"Reset the KV cache between sequences\"\"\"\n",
    "        if self.cache_k is not None and self.cache_v is not None:\n",
    "            self.cache_k.zero()\n",
    "            self.cache_v.zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6413bd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output shape: torch.Size([2, 10, 32])\n",
      "torch.Size([2, 5, 2, 4])\n",
      "torch.Size([2, 16, 2, 4])\n",
      "torch.Size([2, 8, 5, 4])\n",
      "torch.Size([2, 8, 5, 4])\n",
      "Inference output shape: torch.Size([2, 5, 32])\n",
      "\n",
      "✅ Test passed: Training and Inference outputs are correct!\n"
     ]
    }
   ],
   "source": [
    "def test_attention_with_kv_cache():\n",
    "    torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "    # Model params\n",
    "    dim = 32\n",
    "    num_heads = 8\n",
    "    num_kv_heads = 2\n",
    "    max_seq_len = 16\n",
    "\n",
    "    # Make model\n",
    "    model = AttentionWithKVCache(dim, num_heads, max_seq_len, num_kv_heads)\n",
    "\n",
    "    # ---------------- Training mode ----------------\n",
    "    model.train()\n",
    "    x_train = torch.randn(2, 10, dim)  # (batch_size=2, seq_len=10, dim)\n",
    "\n",
    "    out_train = model(x_train)\n",
    "    print(\"Training output shape:\", out_train.shape)\n",
    "\n",
    "    assert out_train.shape == (2, 10, dim), \"Training output has wrong shape!\"\n",
    "\n",
    "    # ---------------- Inference mode ----------------\n",
    "    model.eval()\n",
    "    model.reset_cache()\n",
    "\n",
    "    x_infer = torch.randn(2, 5, dim)  # (batch_size=2, seq_len=5, dim)\n",
    "    out_infer = model(x_infer, start_pos=0)\n",
    "    print(\"Inference output shape:\", out_infer.shape)\n",
    "\n",
    "    assert out_infer.shape == (2, 5, dim), \"Inference output has wrong shape!\"\n",
    "\n",
    "    print(\"\\n✅ Test passed: Training and Inference outputs are correct!\")\n",
    "\n",
    "test_attention_with_kv_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d71c0",
   "metadata": {},
   "source": [
    "## Adding sliding window to gqa,kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbb9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5f3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "class AttentionWithKVCache(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, window_size: int, max_seq_len: int = 2048, num_kv_heads:int=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_kv_heads=num_kv_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.repeats=num_heads//num_kv_heads\n",
    "        self.window_size=window_size\n",
    "        self.half_window=self.window_size//2\n",
    "        \n",
    "        # Projection layers\n",
    "        self.W_q = nn.Linear(dim,self.num_heads*self.head_dim)\n",
    "        self.W_k = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_v = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "\n",
    "        # Initialize KV cache\n",
    "        # self.register_buffer('cache_k', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)  # batch=1 for simplicity\n",
    "        # ))\n",
    "        # self.register_buffer('cache_v', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)\n",
    "        # ))\n",
    "        \n",
    "        self.cache_k=None\n",
    "        self.cache_v=None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project queries, keys, values\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            # Training mode - full attention\n",
    "            q=q.transpose(1,2)\n",
    "            k=k.transpose(1,2)\n",
    "            v=v.transpose(1,2)\n",
    "            k=torch.repeat_interleave(k,dim=1,repeats=self.repeats) #batch_size,num_heads,seq_len,head_dim\n",
    "            v=torch.repeat_interleave(v,dim=1,repeats=self.repeats)#batch_size,num_heads,seq_len,head_dim\n",
    "           \n",
    "            pad_k=F.pad(k,(0,0,self.half_window,self.half_window))\n",
    "            pad_v=F.pad(v,(0,0,self.half_window,self.half_window))\n",
    "\n",
    "            k_unf=pad_k.unfold(dimension=2,size=self.window_size,step=1).transpose(3,4) #(batch_size,num_heads,seq_len,self.window_size,d_head)\n",
    "            v_unf=pad_v.unfold(dimension=2,size=self.window_size,step=1).transpose(3,4) #(batch_size,num_heads,seq_len,self.window_size,d_head)\n",
    "            q=q.unsqueeze(-2)\n",
    "            \n",
    "            attn_scores=einops.einsum(q,k_unf,'b h s w d, b h s w d -> b h s w ')\n",
    "            \n",
    "            mask=torch.tril(torch.ones(self.window_size,self.window_size,device=device))\n",
    "            mask=mask[-1]\n",
    "            mask=mask.view(1,1,1,self.window_size).expand(batch_size,self.num_heads,seq_len,self.window_size)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "            output=einops.einsum(attn_weights,v_unf,'b h s w, b h s w d ->b h s d')\n",
    "            \n",
    "        else:\n",
    "            # Inference mode - use KV cache\n",
    "            # Update cache\n",
    "            if self.cache_k is None or self.cache_v is None:\n",
    "                self.cache_k=torch.zeros((batch_size,self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "                self.cache_v=torch.zeros((batch_size,self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "            print(k.shape)\n",
    "            print(self.cache_k.shape)\n",
    "            self.cache_k[:batch_size, start_pos:start_pos+seq_len] = k\n",
    "            self.cache_v[:batch_size, start_pos:start_pos+seq_len] = v\n",
    "            \n",
    "            # Get keys/values up to current position\n",
    "\n",
    "            keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
    "            \n",
    "            q=q.transpose(1,2)\n",
    "            keys=keys.transpose(1,2)\n",
    "            values=values.transpose(1,2)            # Compute attention\n",
    "            keys=keys.repeat_interleave(self.repeats,dim=1)\n",
    "            values=values.repeat_interleave(self.repeats,dim=1)\n",
    "            print(keys.shape)\n",
    "            print(q.shape)\n",
    "            attn_scores = torch.matmul(q, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # Apply causal mask for new tokens\n",
    "            if seq_len > 1:  # Only needed when processing multiple new tokens\n",
    "                mask = torch.ones((seq_len, start_pos+seq_len), dtype=torch.bool, device=x.device)\n",
    "                mask = torch.tril(mask, diagonal=start_pos)\n",
    "                attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, values)\n",
    "\n",
    "        # Output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "\n",
    "    def reset_cache(self):\n",
    "        \"\"\"Reset the KV cache between sequences\"\"\"\n",
    "        if self.cache_k is not None and self.cache_v is not None:\n",
    "            self.cache_k.zero()\n",
    "            self.cache_v.zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbe879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dl_papers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "dim = 32\n",
    "num_heads = 4\n",
    "num_kv_heads = 2\n",
    "window_size = 5\n",
    "\n",
    "model = AttentionWithKVCache(dim=dim, num_heads=num_heads, num_kv_heads=num_kv_heads, window_size=window_size)\n",
    "model.train()\n",
    "model=model.to(device)\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "x=x.to(device)\n",
    "out = model(x)\n",
    "\n",
    "print(out.shape)\n",
    "# Should print: (2, 10, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789762f0",
   "metadata": {},
   "source": [
    "## Adding Rolling buffer cache to gqa,sliding window,kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f4874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After inserting data 1:\n",
      "[tensor([ 1.5755, -2.1257, -1.3995])]\n",
      "After inserting data 2:\n",
      "[tensor([ 1.5755, -2.1257, -1.3995]), tensor([ 0.2451, -1.1397,  0.7424])]\n",
      "After inserting data 3:\n",
      "[tensor([ 1.5755, -2.1257, -1.3995]), tensor([ 0.2451, -1.1397,  0.7424]), tensor([-1.8523, -0.0405, -0.0153])]\n",
      "After inserting data 4:\n",
      "[tensor([ 1.5755, -2.1257, -1.3995]), tensor([ 0.2451, -1.1397,  0.7424]), tensor([-1.8523, -0.0405, -0.0153]), tensor([-0.4123, -1.3378, -1.3015])]\n",
      "After inserting data 5:\n",
      "[tensor([ 1.5755, -2.1257, -1.3995]), tensor([ 0.2451, -1.1397,  0.7424]), tensor([-1.8523, -0.0405, -0.0153]), tensor([-0.4123, -1.3378, -1.3015]), tensor([0.6482, 1.3867, 0.8134])]\n",
      "After inserting data 6:\n",
      "[tensor([ 0.2451, -1.1397,  0.7424]), tensor([-1.8523, -0.0405, -0.0153]), tensor([-0.4123, -1.3378, -1.3015]), tensor([0.6482, 1.3867, 0.8134]), tensor([ 1.0984, -0.2068, -0.2628])]\n",
      "After inserting data 7:\n",
      "[tensor([-1.8523, -0.0405, -0.0153]), tensor([-0.4123, -1.3378, -1.3015]), tensor([0.6482, 1.3867, 0.8134]), tensor([ 1.0984, -0.2068, -0.2628]), tensor([ 0.0846, -0.1699, -0.3019])]\n"
     ]
    }
   ],
   "source": [
    "class RollingBufferCache:\n",
    "    def __init__(self,max_size:int,dim:int):\n",
    "        self.max_size=max_size\n",
    "        self.dim=dim\n",
    "        self.cache=[]\n",
    "        \n",
    "    \n",
    "    def insert(self,new_data):\n",
    "        if len(self.cache)>=self.max_size:\n",
    "            self.cache.pop(0)\n",
    "        self.cache.append(new_data)\n",
    "        #self.index=(self.index+1) % self.max_size\n",
    "    \n",
    "    def get_cache(self):\n",
    "        return self.cache\n",
    "\n",
    "\n",
    "max_size=5\n",
    "dim=3\n",
    "cache=RollingBufferCache(max_size,dim)\n",
    "\n",
    "for i in range(7):\n",
    "    data=torch.randn(dim)\n",
    "    cache.insert(data)\n",
    "    print(f\"After inserting data {i+1}:\")\n",
    "    print(cache.get_cache())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bff3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "class AttentionWithKVCache(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, window_size: int, max_seq_len: int = 2048, num_kv_heads:int=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.num_kv_heads=num_kv_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.repeats=num_heads//num_kv_heads\n",
    "        self.window_size=window_size\n",
    "        self.half_window=self.window_size//2\n",
    "        \n",
    "        # Projection layers\n",
    "        self.W_q = nn.Linear(dim,self.num_heads*self.head_dim)\n",
    "        self.W_k = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_v = nn.Linear(dim, self.num_kv_heads*self.head_dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "\n",
    "        # Initialize KV cache\n",
    "        # self.register_buffer('cache_k', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)  # batch=1 for simplicity\n",
    "        # ))\n",
    "        # self.register_buffer('cache_v', torch.zeros(\n",
    "        #     (1, max_seq_len, self.num_kv_heads, self.head_dim)\n",
    "        # ))\n",
    "        self.register_buffer('cache_k', torch.zeros((max_seq_len, self.num_kv_heads, self.head_dim)))\n",
    "        self.register_buffer('cache_v', torch.zeros((max_seq_len, self.num_kv_heads, self.head_dim)))\n",
    "        \n",
    "        # self.cache_k=None\n",
    "        # self.cache_v=None\n",
    "        self.cache_pos=0\n",
    "    \n",
    "    def update_cache(self,seq_len,k,v):\n",
    "        seq_len=k.size(1)\n",
    "        \n",
    "        if self.cache_pos + seq_len > self.max_seq_len: #check if cache has enough space\n",
    "            #roll the cache to make space\n",
    "            roll_amount=seq_len\n",
    "            self.cache_k=torch.roll(self.cache_k,shifts=-roll_amount,dim=0)\n",
    "            self.cache_v=torch.roll(self.cache_v,shifts=-roll_amount,dim=0)\n",
    "            self.cache_pos-=roll_amount\n",
    "        \n",
    "        self.cache_k[self.cache_pos:self.cache_pos+seq_len]=k.squeeze(0)\n",
    "        self.cache_v[self.cache_pos:self.cache_pos+seq_len]=v.squeeze(0)\n",
    "        self.cache_pos+=seq_len\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = 0):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project queries, keys, values\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            # Training mode - full attention\n",
    "            q=q.transpose(1,2)\n",
    "            k=k.transpose(1,2)\n",
    "            v=v.transpose(1,2)\n",
    "            k=torch.repeat_interleave(k,dim=1,repeats=self.repeats) #batch_size,num_heads,seq_len,head_dim\n",
    "            v=torch.repeat_interleave(v,dim=1,repeats=self.repeats)#batch_size,num_heads,seq_len,head_dim\n",
    "           \n",
    "            pad_k=F.pad(k,(0,0,self.half_window,self.half_window))\n",
    "            pad_v=F.pad(v,(0,0,self.half_window,self.half_window))\n",
    "\n",
    "            k_unf=pad_k.unfold(dimension=2,size=self.window_size,step=1).transpose(3,4) #(batch_size,num_heads,seq_len,self.window_size,d_head)\n",
    "            v_unf=pad_v.unfold(dimension=2,size=self.window_size,step=1).transpose(3,4) #(batch_size,num_heads,seq_len,self.window_size,d_head)\n",
    "            q=q.unsqueeze(-2)\n",
    "            \n",
    "            attn_scores=einops.einsum(q,k_unf,'b h s w d, b h s w d -> b h s w ')\n",
    "            \n",
    "            mask=torch.tril(torch.ones(self.window_size,self.window_size,device=device))\n",
    "            mask=mask[-1]\n",
    "            mask=mask.view(1,1,1,self.window_size).expand(batch_size,self.num_heads,seq_len,self.window_size)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "            output=einops.einsum(attn_weights,v_unf,'b h s w, b h s w d ->b h s d')\n",
    "            \n",
    "        else:\n",
    "            #batch_size must be 1 for inference\n",
    "            assert batch_size==1, \"batch size must be 1\"\n",
    "            # Inference mode - use KV cache\n",
    "            # Update cache\n",
    "            self.update_cache(seq_len,k,v)\n",
    "            current_len=min(self.cache_pos,self.max_seq_len)\n",
    "            valid_cache_len=min(current_len,self.window_size)\n",
    "            start_window=max(0,current_len-seq_len-self.half_window)\n",
    "                \n",
    "            if self.cache_k is None or self.cache_v is None:\n",
    "                self.cache_k=torch.zeros((self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "                self.cache_v=torch.zeros((self.max_seq_len,self.num_kv_heads,self.head_dim))\n",
    "            \n",
    "            cached_k=self.cache_k[start_window:current_len].unsqueeze(0)\n",
    "            cached_v=self.cache_v[start_window:current_len].unsqueeze(0)\n",
    "            \n",
    " \n",
    "            \n",
    "            print(k.shape)\n",
    "            print(self.cache_k.shape)\n",
    "            \n",
    "\n",
    "            q=q.transpose(1,2)\n",
    "            cached_k=cached_k.transpose(1,2)\n",
    "            cached_v=cached_v.transpose(1,2)            # Compute attention\n",
    "            cached_k=cached_k.repeat_interleave(self.repeats,dim=1)\n",
    "            cached_v=cached_v.repeat_interleave(self.repeats,dim=1)\n",
    "           \n",
    "            attn_scores = torch.matmul(q, cached_k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            \n",
    "            # Apply causal mask for new tokens\n",
    "        \n",
    "            mask = torch.ones((seq_len, valid_cache_len), dtype=torch.bool, device=x.device)\n",
    "            mask = torch.tril(mask, diagonal=valid_cache_len - seq_len)\n",
    "\n",
    "            # Expand mask to (batch_size, num_heads, seq_len, valid_cache_len)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)  # (1,1,seq_len,valid_cache_len)\n",
    "            mask = mask.expand(batch_size, model.num_heads, seq_len, valid_cache_len)\n",
    "\n",
    "            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            output = torch.matmul(attn_probs, cached_v)\n",
    "\n",
    "        # Output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "\n",
    "    def reset_cache(self):\n",
    "        \"\"\"Reset the KV cache between sequences\"\"\"\n",
    "        self.cache_k.zero_()\n",
    "        self.cache_v.zero_()\n",
    "        self.cache_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa0ffcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def test_attention_inference_loop():\n",
    "    # Setup parameters\n",
    "    dim = 16\n",
    "    num_heads = 4\n",
    "    num_kv_heads = 2\n",
    "    window_size = 6\n",
    "    max_seq_len = 20\n",
    "\n",
    "    # Create the model\n",
    "    model = AttentionWithKVCache(\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        num_kv_heads=num_kv_heads,\n",
    "        window_size=window_size,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    model.eval()  # Important! Set to inference mode\n",
    "    model.reset_cache()\n",
    "    batch_size = 1\n",
    "    seq_len = 3\n",
    "    input_tensor = torch.randn(batch_size, seq_len, dim)\n",
    "\n",
    "    # Save old cache state\n",
    "    old_cache_pos = model.cache_pos\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, dim), \"Output shape mismatch\"\n",
    "\n",
    "    # Check cache update\n",
    "    assert model.cache_pos == old_cache_pos + seq_len, \"Cache position not updated correctly\"\n",
    "    assert torch.all(model.cache_k[old_cache_pos:old_cache_pos+seq_len] != 0), \"Keys not updated in cache\"\n",
    "    assert torch.all(model.cache_v[old_cache_pos:old_cache_pos+seq_len] != 0), \"Values not updated in cache\"\n",
    "\n",
    "    # Additional checks\n",
    "    # Test that values outside updated range are still zero\n",
    "    if old_cache_pos > 0:\n",
    "        assert torch.all(model.cache_k[0:old_cache_pos] == 0) or torch.all(model.cache_v[0:old_cache_pos] == 0), \"Previous cache values modified incorrectly\"\n",
    "\n",
    "    # Make another forward pass to check rolling\n",
    "    input_tensor2 = torch.randn(batch_size, seq_len, dim)\n",
    "    output2 = model(input_tensor2)\n",
    "\n",
    "    # Output2 shape\n",
    "    assert output2.shape == (batch_size, seq_len, dim), \"Second output shape mismatch after second inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f413bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2, 4])\n",
      "torch.Size([20, 2, 4])\n",
      "torch.Size([1, 3, 2, 4])\n",
      "torch.Size([20, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "test_attention_inference_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "455e5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "dim = 32\n",
    "num_heads = 4\n",
    "num_kv_heads = 2\n",
    "window_size = 5\n",
    "\n",
    "model = AttentionWithKVCache(dim=dim, num_heads=num_heads, num_kv_heads=num_kv_heads, window_size=window_size)\n",
    "model.train()\n",
    "model=model.to(device)\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "x=x.to(device)\n",
    "out = model(x)\n",
    "\n",
    "print(out.shape)\n",
    "# Should print: (2, 10, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b3c00",
   "metadata": {},
   "source": [
    "## Sparse mixture of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "900e257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    # a single expert\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.w1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.w2=nn.Linear(input_dim,hidden_dim)\n",
    "        self.out=nn.Linear(hidden_dim,input_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.out(self.w1(x) * F.silu(self.w2(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2413f41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.randn(2*10,32)\n",
    "\n",
    "torch.topk(x,k=2,dim=-1)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "475007ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self,d_model,d_hidden,num_experts=8,top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts=num_experts\n",
    "        self.top_k=top_k\n",
    "        \n",
    "        self.experts=nn.ModuleList(\n",
    "            [\n",
    "                SwiGLUFFN(input_dim=d_model,hidden_dim=d_hidden)\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.router=nn.Linear(input_dim,num_experts)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        batch_size,seq_len,d_model=x.shape\n",
    "        x_flat=x.view(-1,d_model) # (batch_size * seq_len, d_model)\n",
    "        \n",
    "        \n",
    "        #Step 1: get router scores for each token\n",
    "        router_logits=self.router(x_flat)\n",
    "        router_probs=F.softmax(router_logits,dim=-1)\n",
    "        \n",
    "        #Step 2: get top-k experts\n",
    "        topk_probs,topk_indices=torch.topk(router_probs,self.top_k,dim=-1) #(batch_size*seq_len, top_k)\n",
    "        \n",
    "        #Step 3: Compute outputs from selected experts\n",
    "        expert_outputs=[]\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx=topk_indices[:,i] \n",
    "            outputs=torch.zeros_like(x_flat)\n",
    "            \n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask=(expert_id==expert_idx)\n",
    "                if mask.any():\n",
    "                    selected_x=x_flat[mask]\n",
    "                    expert_out=self.experts[expert_id](selected_x)\n",
    "                    outputs[mask]=expert_out\n",
    "            \n",
    "            weighted_output = topk_probs[:, i].unsqueeze(-1) * outputs\n",
    "            expert_outputs.append(weighted_output)\n",
    "\n",
    "        \n",
    "        # 4. Sum the expert outputs\n",
    "        final_output = sum(expert_outputs)\n",
    "\n",
    "        # 5. Reshape back to (batch_size, seq_len, d_model)\n",
    "        final_output = final_output.view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        router_probs_mean = router_probs.mean(dim=0)\n",
    "        load_balancing_loss = (router_probs_mean * router_probs_mean).sum() * self.num_experts\n",
    "\n",
    "        return final_output, load_balancing_loss\n",
    "    \n",
    "#final_loss = task_loss + router_loss_weight * router_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc741136",
   "metadata": {},
   "source": [
    "## ROPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def precompute_theta_pos_frequencies(d_head,seq_len,theta=1000.0):\n",
    "    assert d_head %2==0\n",
    "    theta_nr=torch.arange(0,head_dim,2).float()\n",
    "    theta=1.0 / ( theta**(theta_nr/d_head))\n",
    "    m=torch.arange(seq_len)\n",
    "    \n",
    "    freqs=torch.outer(m,theta).float()\n",
    "    \n",
    "    freqs_complex=torch.polar(torch.ones_like(freqs),freqs)\n",
    "    \n",
    "    return freqs_complex\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(x,freqs_complex):\n",
    "    x_complex=torch.view()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
